{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmiGm-hILZg5"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_mushroom_list(url):\n",
        "    data = requests.get(url).text\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "    # Trouver tous les liens vers les pages individuelles des champignons\n",
        "    mushroom_links = soup.find_all('a', href=re.compile(r'/show\\?n='))\n",
        "    mushroom_urls = [urljoin('http://www.mushroom.world', link['href']) for link in mushroom_links]\n",
        "\n",
        "    # Récupérer les informations pour chaque champignon\n",
        "    mushrooms = [scrape_mushroom(link) for link in mushroom_urls]\n",
        "\n",
        "    return mushrooms\n",
        "\n",
        "def scrape_mushroom(url):\n",
        "    data = requests.get(url).text\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "    # Extraire les informations spécifiques pour un champignon individuel\n",
        "    name_content = soup.find(class_=\"caption\").find(\"b\").contents\n",
        "    names = re.sub('[^A-Za-z0-9( ]+', '', name_content[0]).split(\"(\")\n",
        "    names = [n.strip() for n in names]\n",
        "    name1 = names[0]\n",
        "    name2 = names[1] if len(names) > 1 else ''\n",
        "\n",
        "    labels = soup.find_all(class_=\"labelus\")\n",
        "    labels = [label.contents[0] for label in labels]\n",
        "\n",
        "    texts = soup.find_all(class_=\"textus\")\n",
        "    texts = [text.contents[0] for text in texts]\n",
        "\n",
        "    description = soup.find(class_=\"longtextus\").contents\n",
        "    description = [re.sub('[^A-Za-z0-9,.<> ]+', '', str(d)).strip() for d in description]\n",
        "    description = [re.sub('<b>', '', d) for d in description if (d != \"\") & (d != \"<br>\")]\n",
        "    description.insert(0, 'General')\n",
        "    description = dict(zip(description[0::2], description[1::2]))\n",
        "\n",
        "    texts.append(description)\n",
        "    assert len(labels) == len(texts)\n",
        "\n",
        "    images = soup.find(id=\"mushroom-list\").find_all(class_=\"image\")\n",
        "    image_urls = [urljoin('http://www.mushroom.world', image.a[\"href\"]) for image in images]\n",
        "\n",
        "    mushroom = dict(name1=name1, name2=name2, images=image_urls, info=dict())\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        mushroom[\"info\"][labels[i]] = texts[i]\n",
        "\n",
        "    return mushroom\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Lien vers la liste de tous les champignons sur mushroom.world\n",
        "    all_mushrooms_url = 'http://www.mushroom.world/mushrooms/namelist'\n",
        "\n",
        "    # Récupérer les informations sur tous les champignons\n",
        "    all_mushrooms = scrape_mushroom_list(all_mushrooms_url)\n",
        "\n",
        "    # Afficher les informations (ou les enregistrer dans un fichier, etc.)\n",
        "    print(json.dumps(all_mushrooms))"
      ]
    }
  ]
}